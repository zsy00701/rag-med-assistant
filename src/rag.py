import os
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# ================= é…ç½® =================
# Chroma Cloud é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
CHROMA_API_KEY = os.getenv("CHROMA_API_KEY")
CHROMA_TENANT = os.getenv("CHROMA_TENANT", "default_tenant")
CHROMA_DATABASE = os.getenv("CHROMA_DATABASE", "default_database")
CHROMA_COLLECTION = os.getenv("CHROMA_COLLECTION", "medical_qa")

# æœ¬åœ°æ•°æ®åº“é…ç½®ï¼ˆå›é€€é€‰é¡¹ï¼‰
DB_PATH = "./chroma_db_medical"
# =======================================

def get_rag_chain():
    embedding_model = OpenAIEmbeddings()
    
    # 2. åŠ è½½æ•°æ®åº“ï¼ˆä¼˜å…ˆä½¿ç”¨ Chroma Cloudï¼Œå¦åˆ™ä½¿ç”¨æœ¬åœ°ï¼‰
    if CHROMA_API_KEY:
        # ä½¿ç”¨ Chroma Cloud
        try:
            import chromadb
            print("â˜ï¸  ä½¿ç”¨ Chroma Cloud è¿æ¥...")
            chroma_client = chromadb.CloudClient(
                api_key=CHROMA_API_KEY,
                tenant=CHROMA_TENANT,
                database=CHROMA_DATABASE
            )
            vectorstore = Chroma(
                client=chroma_client,
                collection_name=CHROMA_COLLECTION,
                embedding_function=embedding_model
            )
            print(f"âœ… å·²è¿æ¥åˆ° Chroma Cloud: {CHROMA_DATABASE}/{CHROMA_COLLECTION}")
        except Exception as e:
            print(f"âš ï¸  Chroma Cloud è¿æ¥å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æœ¬åœ°æ•°æ®åº“...")
            # å›é€€åˆ°æœ¬åœ°
            if not os.path.exists(DB_PATH):
                raise FileNotFoundError("âŒ å‘é‡æ•°æ®åº“æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ src/ingest.py")
            vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embedding_model)
    else:
        # ä½¿ç”¨æœ¬åœ°æ•°æ®åº“
        print("ğŸ’¾ ä½¿ç”¨æœ¬åœ°æ•°æ®åº“...")
        if not os.path.exists(DB_PATH):
            raise FileNotFoundError("âŒ å‘é‡æ•°æ®åº“æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ src/ingest.py")
        vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embedding_model)
    
    # 3. åˆ›å»ºæ£€ç´¢å™¨
    # k=3: æ¯æ¬¡æ‰¾ 3 æ¡æœ€ç›¸å…³çš„ QA ç»™å¤§æ¨¡å‹å‚è€ƒ
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 4. åˆå§‹åŒ–å¤§æ¨¡å‹
    # temperature=0: åŒ»ç–—å»ºè®®å¿…é¡»ä¸¥è°¨ï¼Œä¸è¦å‘æ•£
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    # 5. å®šä¹‰ Prompt (è§’è‰²è®¾å®š)
    template = """
    ä½ æ˜¯ä¸€åä¸“ä¸šçš„è‚èƒ†èƒ°å¤–ç§‘æœ¯ååº·å¤åŠ©æ‰‹ã€‚
    è¯·åŸºäºä»¥ä¸‹ã€å·²çŸ¥ä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    
    è¦æ±‚ï¼š
    1. è¯­æ°”ä¸“ä¸šã€äº²åˆ‡ã€å¯Œæœ‰åŒç†å¿ƒã€‚
    2. å¦‚æœã€å·²çŸ¥ä¿¡æ¯ã€‘ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥â€œèµ„æ–™åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå»ºè®®å’¨è¯¢ä¸»æ²»åŒ»ç”Ÿâ€ï¼Œä¸¥ç¦çç¼–ã€‚
    3. ç­”æ¡ˆè¦æ¡ç†æ¸…æ™°ã€‚

    ã€å·²çŸ¥ä¿¡æ¯ã€‘ï¼š
    {context}

    ç”¨æˆ·é—®é¢˜ï¼š{question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    # 6. æ„é€ é“¾
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

def start_chat():
    if "OPENAI_API_KEY" not in os.environ:
        print("âŒ é”™è¯¯: è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
        return

    print("ğŸ¥ è‚èƒ†èƒ°æœ¯ååº·å¤åŠ©æ‰‹æ­£åœ¨å¯åŠ¨...")
    try:
        chain = get_rag_chain()
        print("âœ… ç³»ç»Ÿå°±ç»ªï¼è¯·è¾“å…¥é—®é¢˜ (è¾“å…¥ 'q' é€€å‡º)")
        print("-" * 50)
        
        while True:
            query = input("\nğŸ‘¤ æ‚£è€…æé—®: ")
            if query.lower() in ['q', 'quit', 'exit']:
                break
            
            if not query.strip():
                continue

            print("ğŸ¤– æ­£åœ¨æ€è€ƒ...", end="", flush=True)
            start_time = time.time()
            
            # è°ƒç”¨é“¾
            response = chain.invoke(query)
            
            end_time = time.time()
            print(f"\rğŸ¤– åŠ©æ‰‹å›ç­” ({end_time - start_time:.2f}s):")
            print(response)
            print("-" * 50)

    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    start_chat()